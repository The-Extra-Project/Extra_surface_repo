# ------------------------------------------------------------------------------------ #
#                                     JVM & libhdfs.so
# ------------------------------------------------------------------------------------ #

FROM --platform=linux/amd64 ubuntu:20.04 as hdfs

ENV DEBIAN_FRONTEND="noninteractive"
RUN mkdir -p /run/systemd && echo 'docker' > /run/systemd/container

USER root

RUN apt-get update && apt-get install -y wget gpg

# JDK 8 AmazonCorretto
RUN wget -O - https://apt.corretto.aws/corretto.key | gpg --dearmor -o /usr/share/keyrings/corretto-keyring.gpg && \
    echo "deb [signed-by=/usr/share/keyrings/corretto-keyring.gpg] https://apt.corretto.aws stable main" | \
    tee /etc/apt/sources.list.d/corretto.list

RUN apt-get update && apt-get install -y \
    unzip git curl \
    # JVM
    java-1.8.0-amazon-corretto-jdk \
    # Hadoop
    # libssl-dev
    autoconf automake libtool g++ cmake zlib1g-dev pkg-config \
    libssl-dev libsasl2-dev libsnappy-dev \
    bzip2 libbz2-dev fuse libfuse-dev maven \
    && apt-get clean
    # && rm -rf /var/lib/apt/lists/*

WORKDIR /usr/local/bin

# **<***>**<***> Hadoop **<***>**<***>

ARG V_HADOOP=3.3.6
ENV JAVA_HOME=/usr/lib/jvm/java-1.8.0-amazon-corretto
ENV LD_LIBRARY_PATH=/usr/local/lib

# Protobuf for Hadoop build
RUN curl -L -s -S https://github.com/protocolbuffers/protobuf/releases/download/v3.7.1/protobuf-java-3.7.1.tar.gz \
    -o protobuf-3.7.1.tar.gz && mkdir protobuf-3.7-src && \
    tar xzf protobuf-3.7.1.tar.gz --strip-components 1 -C protobuf-3.7-src && \
    cd protobuf-3.7-src && \ 
    ./configure && make -j2 && make install

# Download Hadoop
RUN wget https://archive.apache.org/dist/hadoop/core/hadoop-${V_HADOOP}/hadoop-${V_HADOOP}-src.tar.gz && \
    tar -xf hadoop-${V_HADOOP}-src.tar.gz && rm ./*gz

# Compile libhdfs.so
RUN cd hadoop-${V_HADOOP}-src/hadoop-hdfs-project && \
    mvn package -Pnative -DskipTests

# Prepare the files for the next stage
RUN mv hadoop-${V_HADOOP}-src/hadoop-hdfs-project/hadoop-hdfs-native-client/target/native/target/usr/local/lib/libhdfs.so.0.0.0 \
    /opt/libhdfs.so

RUN rm -f ${JAVA_HOME}/src.zip ${JAVA_HOME}/javafx-src.zip && \
    mkdir -p /opt/jdk && mv $JAVA_HOME/* /opt/jdk


# ------------------------------------------------------------------------------------ #
#                                       Build App
# ------------------------------------------------------------------------------------ #

FROM --platform=linux/amd64 python:3.9-bullseye as build 
# bullseye for prod compat

ENV DEBIAN_FRONTEND="noninteractive"
RUN mkdir -p /run/systemd && echo 'docker' > /run/systemd/container

USER root

# **<***>**<***> Libs **<***>**<***>

# JDK 8 AmazonCorretto
RUN wget -O - https://apt.corretto.aws/corretto.key | gpg --dearmor -o /usr/share/keyrings/corretto-keyring.gpg && \
    echo "deb [signed-by=/usr/share/keyrings/corretto-keyring.gpg] https://apt.corretto.aws stable main" | \
    tee /etc/apt/sources.list.d/corretto.list

RUN apt-get update && apt-get install -y \
    cmake build-essential \
    # Cgal
    libgmp-dev libmpfr-dev \
    java-1.8.0-amazon-corretto-jdk \
    # Lidar format dps
    libxerces-c-dev xsdcxx \
    # Matis-lib dps
    libeigen3-dev imagemagick \
    # CGAL et al
    # libboost-all-dev libboost-filesystem-dev \
    # libcgal-qt5-dev libcgal-dev \
    libtinyxml-dev libann-dev libtbb-dev \
    # PyMesh
    # python3 python3-dev python3-pip \
    # Hadoop
    && apt-get clean
    # && rm -rf /var/lib/apt/lists/*

COPY --from=hdfs /opt/libhdfs.so /usr/local/lib/libhdfs.so

ENV JAVA_HOME=/usr/lib/jvm/java-1.8.0-amazon-corretto

ENV JOBS=2

# **<***>**<***> Scala, Spark **<***>**<***>
ENV V_SCALA=2.12.8 \
    V_SBT=1.0.0 \
    V_SPARK=3.5.1

WORKDIR /usr/local/bin

# Scala
RUN wget https://downloads.lightbend.com/scala/${V_SCALA}/scala-${V_SCALA}.deb && \
    dpkg -i scala-${V_SCALA}.deb && apt-get install -f && rm scala-${V_SCALA}.deb
# Sbt
RUN wget https://github.com/sbt/sbt/releases/download/v${V_SBT}/sbt-${V_SBT}.tgz && \
    tar -xf sbt-${V_SBT}.tgz && rm ./*gz
# Spark
RUN wget https://archive.apache.org/dist/spark/spark-${V_SPARK}/spark-${V_SPARK}-bin-hadoop3-scala2.13.tgz && \
    tar -xf spark-${V_SPARK}-bin-hadoop3-scala2.13.tgz && rm ./*gz

# **<***>**<***> C++ libraries **<***>**<***>

ENV P_XML=/cpp/libs/xml \
    P_CGAL=/cpp/libs/cgal \
    P_BOOST=/cpp/libs/boost

# LibXML
RUN wget https://download.gnome.org/sources/libxml2/2.13/libxml2-2.13.4.tar.xz && mkdir -p ${P_XML} && \
    tar -xf libxml2-2.13.4.tar.xz -C $P_XML --strip-components=1 && rm ./*.xz && cd $P_XML && \
    ./configure --with-legacy --prefix=${P_XML}/i && make -j $JOBS && make install

# CGAL
RUN wget https://github.com/CGAL/cgal/releases/download/v5.6.1/CGAL-5.6.1.tar.xz && mkdir -p ${P_CGAL} && \
    tar -xf CGAL-5.6.1.tar.xz -C $P_CGAL --strip-components=1 && rm ./*.xz && \
    mkdir -p $P_CGAL/build && cd $P_CGAL/build && \
    cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=${P_CGAL}/i .. && \
    make -j $JOBS && make install

# Boost
RUN wget https://archives.boost.io/release/1.86.0/source/boost_1_86_0.tar.gz && mkdir -p ${P_BOOST} && \
    tar -xf boost_1_86_0.tar.gz -C $P_BOOST --strip-components=1 && rm ./*.gz

RUN cd $P_BOOST && ./bootstrap.sh --prefix=$P_BOOST/i && ./b2 install

RUN cp -r ${P_XML}/i/lib/* /usr/local/lib/ && \
    cp -r ${P_BOOST}/i/lib/* /usr/local/lib/ && \
    cp -r ${P_CGAL}/i/include/* /usr/include/ && \
    cp -r ${P_XML}/i/include/* /usr/include/ && \
    cp -r ${P_BOOST}/i/include/* /usr/include/

ENV CMAKE_PREFIX_PATH=/usr/local/lib:/usr/include/

# ENV LD_LIBRARY_PATH=${P_XML}/i/lib:${P_BOOST}/i/lib:$LD_LIBRARY_PATH \
#     CPLUS_INCLUDE_PATH=${P_XML}/i/include/libxml2:${P_CGAL}/i/include:${P_BOOST}/i/include:$CPLUS_INCLUDE_PATH

# **<***>**<***> Compile DDT, Waseur, Scala binaries **<***>**<***>

WORKDIR /cpp

ENV DDT_TRAITS=3 \
    DOCKER_BUILD_ENV=1

COPY . .

# **<***>**<***> COMMENT OUT ALL LINES BELOW FOR A BUILD IMAGE **<***>**<***>
# In this case you compile the app in your local user namespace
# instead of the container one. This facilitates better caching and build speed.
# Uncomment for the prod, CI deployment.

# RUN ./src/docker/docker_interface.sh compile -j $JOBS

# ------------------------------------------------------------------------------------ #
#                             Python Dependencies (Optional)
# ------------------------------------------------------------------------------------ #

# FROM --platform=linux/amd64 python:3.9-bullseye as py

# ENV PYTHONFAULTHANDLER=1 PYTHONHASHSEED=random \
#     PYTHONUNBUFFERED=1 PIP_DEFAULT_TIMEOUT=100 \
#     PIP_DISABLE_PIP_VERSION_CHECK=1 PIP_NO_CACHE_DIR=1 \
#     DEBIAN_FRONTEND="noninteractive"

# RUN apt-get update && apt-get install -y \
#     # For aarch64 building from source
#     cmake build-essential libgmp-dev python3-distutils \
#     # For pymesh
#     libgl1-mesa-dev libglu1-mesa-dev freeglut3-dev qtbase5-dev \
#     # Py3dtilers
#     libpq-dev

# # RUN git clone --recursive -b v2021.10 https://github.com/cnr-isti-vclab/PyMeshLab.git
# RUN git clone https://github.com/VCityTeam/py3dtilers.git

# # Pymeshlab
# # ARG JOBS="2"
# # RUN mkdir -p PyMeshLab/src/build && cd PyMeshLab/src/build && \
# #    cmake .. && make -j $JOBS && make install && cd ../../..

# # ifcopenshell is unavailable for aarch64, need to explicitly compile from the source
# # fallback to amd64
# COPY ./services/mesh23dtile ./services/mesh23dtile
# RUN cd services/mesh23dtile && pip3 install -r requirements.txt
# RUN cd py3dtilers && pip3 install --no-cache-dir -e .


# ------------------------------------------------------------------------------------ #
#                                      Production
# ------------------------------------------------------------------------------------ #

# FROM --platform=linux/amd64 python:3.9-bullseye as prod
# # python:3.10.15-slim-bullseye as prod

# LABEL Maintainer="Dhruv Malik" Email="dhruv@extralabs.xyz"

# ENV BIN="/usr/local/bin" DEBIAN_FRONTEND=noninteractive \
#     PYTHONFAULTHANDLER=1 PYTHONHASHSEED=random \
#     PYTHONUNBUFFERED=1 PIP_DEFAULT_TIMEOUT=100 \
#     PIP_DISABLE_PIP_VERSION_CHECK=1 PIP_NO_CACHE_DIR=1

# RUN mkdir -p /run/systemd && echo 'docker' > /run/systemd/container

# USER root

# # **<***>**<***> Libs **<***>**<***>
# # RUN wget -O - https://apt.corretto.aws/corretto.key | gpg --dearmor -o /usr/share/keyrings/corretto-keyring.gpg && \
# #     echo "deb [signed-by=/usr/share/keyrings/corretto-keyring.gpg] https://apt.corretto.aws stable main" | tee /etc/apt/sources.list.d/corretto.list

# RUN apt-get update && apt-get install -y --no-install-recommends \
#     # java-1.8.0-amazon-corretto-jdk \
#     # Lidar format dps
#     libxerces-c-dev xsdcxx \
#     # Matis-lib dps
#     libeigen3-dev imagemagick \
#     # CGAL et al
#     libboost-all-dev libboost-filesystem-dev \
#     # libcgal-qt5-dev libcgal-dev \ 
#     libtinyxml-dev libann-dev libtbb-dev \
#     # For py3dtilers
#     libpq-dev \
#     # For pymesh
#     libgl1-mesa-dev libglu1-mesa-dev freeglut3-dev qtbase5-dev \
#     # procps curl \
#     && apt-get clean
#     #&& rm -rf /var/lib/apt/lists/*

# # User hadoop (EMR)
# RUN groupadd hadoop && useradd --gid hadoop --home-dir /home/hadoop --create-home hadoop && \
#     echo "hadoop:hadoop" | chpasswd
# USER hadoop

# # **<***>**<***> App **<***>**<***>
# WORKDIR /app

# # For CI/Prod
# # COPY --from=build /cpp/build ./build

# COPY ./services/mesh23dtile ./services/mesh23dtile
# RUN git clone https://github.com/VCityTeam/py3dtilers.git && cd py3dtilers && \
#     pip3 install --no-cache-dir -e . && cd .. && rm -rf py3dtilers

# RUN cd services/mesh23dtile && pip3 install -r requirements.txt

# COPY . .

# # **<***>**<***> JVM, Spark, Hadoop **<***>**<***>
# COPY --from=build /usr/local/bin/spark-3.5.1-bin-hadoop3-scala2.13    /usr/local/bin/spark
# COPY --from=hdfs /opt/libhdfs.so    /usr/local/lib/libhdfs.so
# COPY --from=hdfs /opt/jdk    /usr/lib/jvm/java-1.8.0
# # COPY --from=hdfs /usr/local/bin/hadoop-3.3.6-src    /usr/local/bin/hadoop

# ENV HADOOP_VERSION=3.3.6 \
#     HADOOP_HOME=${BIN}/hadoop \
#     HADOOP_CONF_DIR=/etc/hadoop/conf 

# ENV HADOOP_COMMON_PATH=${HADOOP_HOME} \
#     HADOOP_HDFS_HOME=${HADOOP_HOME}/hadoop-hdfs-project \
#     HADOOP_MAPRED_HOME=${HADOOP_HOME}/hadoop-mapreduce-project \
#     HADOOP_YARN_HOME=${HADOOP_HOME}/hadoop-yarn-project \
#     # ${HADOOP_HOME}/etc/hadoop
#     SPARK_HOME=${BIN}/spark \
#     JAVA_HOME=/usr/lib/jvm/java-1.8.0

# ENV PATH="${JAVA_HOME}/jre/bin:${HADOOP_HOME}:${SPARK_HOME}/bin/:${PATH}"

# USER root

# Conda
# RUN curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && \
#     bash Miniconda3-latest-Linux-x86_64.sh -b -p ${BIN}/conda && \
#     rm Miniconda3*.sh && conda clean -afy

# RUN conda env create -f ./services/mesh23dtile/environment.yml && \
#     conda clean -afy


# Pymeshlab
# ARG JOBS="2"
# RUN git clone --recursive -b v2021.10 https://github.com/cnr-isti-vclab/PyMeshLab.git
# RUN mkdir -p PyMeshLab/src/build && cd PyMeshLab/src/build && \
#    cmake .. && make -j $JOBS && make install && cd ../../..
# ifcopenshell is unavailable for aarch64, need to explicitly compile from the source
# fallback to amd64


# # AWS CLI
# # RUN wget "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -O "awscliv2.zip" && \
# #     unzip awscliv2.zip && ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update && \
# #     rm -r awscliv2.zip aws
